import os
import pandas as pd
from transformers import (
    T5Tokenizer,
    T5ForConditionalGeneration,
    Trainer,
    TrainingArguments,
)
from datasets import Dataset
import transformers
import accelerate
import torch

# ===== 0Ô∏è‚É£ Version Check =====
print(f"Transformers: {transformers.__version__}, Accelerate: {accelerate.__version__}, Torch: {torch.__version__}")

# ===== 1Ô∏è‚É£ Setup =====
MODEL_NAME = "google/flan-t5-small"
DATA_PATH = "synthetic_dataset.csv"
OUTPUT_DIR = "./Sales_pitch/sales_model"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ===== 2Ô∏è‚É£ Load Dataset =====
print("üì¶ Loading dataset...")
df = pd.read_csv(DATA_PATH).fillna("")
dataset = Dataset.from_pandas(df)
print(f"‚úÖ Loaded {len(df)} samples.")

# ===== 3Ô∏è‚É£ Tokenizer =====
print("üî§ Loading tokenizer...")
tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)

def preprocess(example):
    input_text = (
        f"Product: {example['product_name']}\n"
        f"Description: {example['description']}\n"
        f"Features: {example['features']}\n"
        f"Generate sales recommendation:"
    )
    output_text = (
        f"Target audience: {example['target_audience']}. "
        f"Highlight: {example['highlight_features']}. "
        f"Sales strategy: {example['sales_strategy']}."
    )
    model_inputs = tokenizer(
        input_text,
        truncation=True,
        padding="max_length",
        max_length=128
    )
    labels = tokenizer(
        output_text,
        truncation=True,
        padding="max_length",
        max_length=64
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

print("üß© Tokenizing dataset...")
tokenized = dataset.map(preprocess, batched=False)
print("‚úÖ Tokenization complete.")

# ===== 4Ô∏è‚É£ Model =====
print("‚öôÔ∏è Loading model...")
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)

# Freeze most layers for lightweight training
for param in model.parameters():
    param.requires_grad = False
for param in model.lm_head.parameters():
    param.requires_grad = True

# ===== 5Ô∏è‚É£ Training Args =====
args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=4,
    num_train_epochs=1,
    save_strategy="no",
    logging_steps=5,
    learning_rate=5e-4,
    disable_tqdm=True,
    report_to="none",
    no_cuda=True,  # force CPU mode, avoids accelerate unwrap conflicts
)

# ===== 6Ô∏è‚É£ Trainer =====
print("üöÄ Training started (should take ~2 minutes on CPU)...")

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized,
)

# üöë Clean Accelerate Patch
if hasattr(trainer, "accelerator") and trainer.accelerator is not None:
    orig_unwrap = trainer.accelerator.unwrap_model
    # redefine unwrap_model without unsupported args
    def safe_unwrap_model(model, *_, **__):
        return model
    trainer.accelerator.unwrap_model = safe_unwrap_model

trainer.train()

# ===== 7Ô∏è‚É£ Save Model =====
print("üíæ Saving fine-tuned model...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"‚úÖ Training complete. Model saved to {OUTPUT_DIR}")
